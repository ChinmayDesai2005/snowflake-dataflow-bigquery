Reading data from **Snowflake** and writing to **BigQuery** using an **Apache Beam** pipeline. It can be run using Google Dataflow runner for easy scalability. Apache Beam offers a seamless way to transfer large amounts of data using its Pipeline and PCollection architectures.

How to run it:
You will require access to a Snowflake and Google Cloud BigQuery. After editing code for your credentials, follow the documentation.
Run main.py using python for testing the pipeline locally.

**Documentation:** [Interfacing Snowflake and BigQuery via Apache Beam](https://docs.google.com/document/d/1lUSNj1IwGRni-UsKmKax6EZvVebznGIDQqIFYjuxThU/edit?usp=sharing)
